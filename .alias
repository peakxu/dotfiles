#!/bin/bash

# Git shortcuts
alias git_cleanup="git branch --merged master | egrep -iv '(master|production|staging)' | xargs -n 1 git branch -d"
alias git_cleanup_remote="git branch -r --merged master | egrep -iv '(master|production|staging)' | sed 's/origin\///g' | xargs -n 1 git push --delete origin"
alias grbm='git rebase master'
alias grim='git rebase -i master'
alias gra='git rebase --abort'
alias grc='git rebase --continue'
alias gcm='git commit -m'

# Bundler shortcuts
alias be='bundle exec'
alias ber='be rake'
alias bes='be rspec'
alias bivb='bundle install --path vendor/support'

# Working with data pipeline
alias dp='datapipeline'
alias dpi='datapipeline --id'
alias dpli='datapipeline --list-runs --id'
alias dpla='datapipeline --list-pipelines'
alias dpls='dpla | grep'
alias dplm='dpls peakxu'
alias dplc='dplm | grep FINISHED | awk '/df-.+/{ print $3}' | xargs -L1 datapipeline --delete --list-runs --id'

# Data pipeline tasks
alias dpd='ber definition'
alias dps='ber deploy'
alias dpp='FOG_CREDENTIAL=swipely ber env=production deploy[now]'
alias dpg='ber graph'
alias pg='pipely-graph'

# emr ssh
alias pipe-debug-ssh='aws emr ssh --key-pair-file ~/.ssh/pipeline-debug.pem --cluster-id'

function show-pr {
    export GIT_WORKING_HEAD=`git rev-parse HEAD`
    curl https://api.github.com/repos/dotswipely/weekly_digest_email_pipeline/pulls?access_token=$(< ~/.github_access_token) 2> /dev/null | jq '[.[] | {url: .html_url, sha: .head.sha}]' | sed s/": "/" => "/ | ruby -e "pulls = eval(STDIN.read); pulls.each { |p| puts p['url'] if p['sha'] == ENV['GIT_WORKING_HEAD'] }" | xargs open
}

function fetch-latest-pipeline-logs {
    if [ $# -ne 1 ]; then
        echo "Error, fetch-logs must specify pipeline"
        return 1
    fi
    local pipeline=$1
    if [ $# -ne 3 ]; then
        echo "No environment + owner specified, defaulting to staging + $USER"
        local env='staging'
        local owner="$USER"
    else
        local env=$2
        local owner=$3
    fi
    local s3_logs_path="s3://datapipeline-logs/$env/$owner/$pipeline/"
    local latest_s3_logs="$(s3cmd ls $s3_logs_path | grep DIR | tail -n 1 | tr -s ' ' | cut -d ' ' -f3)"
    latest_s3_logs=${latest_s3_logs%/}
    local timestamp="$(echo $latest_s3_logs | cut -d'/' -f7)"
    local local_logs_path="$HOME/s3/datapipeline-logs/$env/$owner/$pipeline/$timestamp"
    mkdir -p "$local_logs_path"
    aws s3 sync "$latest_s3_logs" "$local_logs_path"
}

function cleanup_hash {
    if [ $# -ne 1 ]; then
        echo "Error, cleanup_hash must specify hash to cleanup"
        return 1
    fi
    local store_hash=$1
    local s3_path="s3://a-default-p-output/$store_hash"
    local folder_name=$(aws s3 ls $s3_path | grep RecentSales | awk '/PRE .+/{ print $2}')
    if [ -z "$folder_name" ]
    then
        return 0
    else
        aws s3 sync /tmp/empty $s3_path --delete
    fi
}
